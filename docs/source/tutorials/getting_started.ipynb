{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(getting_started)=\n",
    "# Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "We're going to demonstrate how to use `hbmep` to estimate recruitment curves from MEP size data using a hierarchical Bayesian model. The model is based on a rectified-logistic function (:cite:alps:`1987:nelson_`, see Methods 4.1.1, Eq. 4.1.4), which was introduced for parametric estimation of the threshold.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    ".. note::\n",
    "\n",
    "    This tutorial assumes that you are working on a Linux or MacOS system. If you are using Windows, you can use the Windows Subsystem for Linux (WSL), or update the bash commands and paths accordingly.\n",
    "\n",
    "Begin by downloading the :file:`mock_data.csv` file from the `hbmep` repository using the following command.\n",
    "\n",
    ".. code-block:: bash\n",
    "\n",
    "    wget https://github.com/hbmep/hbmep/tree/main/docs/source/tutorials/mock_data.csv\n",
    "\n",
    "\n",
    "This file contains transcranial magnetic stimulation (TMS) MEP size data for 3 participants, with responses from two muscles (PKPK_ECR and PKPK_FCR). The responses are recorded in peak-to-peak values in millivolts (mV), and the intensity is recorded in maximum stimulator output (0-100% MSO).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Configuration file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    ":file:`config.toml` is a configuration file used by `hbmep` that contains all the necessary information to run the model.\n",
    "Begin by creating a TOML file, :file:`config.toml`, and enter the following content.\n",
    "\n",
    ".. code-block:: toml\n",
    "\n",
    "    [paths]\n",
    "    csv_path = \"./mock_data.csv\"\n",
    "    build_directory = \"./mock_data_output/\"\n",
    "\n",
    "    [variables]\n",
    "    intensity = \"TMSIntensity\"\n",
    "    features = [\"participant\"]\n",
    "    response = ['PKPK_ECR', 'PKPK_FCR']\n",
    "\n",
    "    [mcmc]\n",
    "    num_chains = 4\n",
    "    num_warmup = 1000\n",
    "    num_samples = 1000\n",
    "    thinning = 1\n",
    "\n",
    "    [misc]\n",
    "    base = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```{eval-rst}\n",
    "Alternatively, you can download this configuration file from the `hbmep` repository by running the following command:\n",
    "\n",
    ".. code-block:: bash\n",
    "\n",
    "    wget https://github.com/hbmep/hbmep/tree/main/docs/source/tutorials/config.toml\n",
    "\n",
    "Here,\n",
    "\n",
    "- ``paths`` table contains the paths to the dataset and the build directory.\n",
    "    - ``csv`` is path to the dataset in csv format. In the TOML file, it points to the :file:`mock_data.csv` file.\n",
    "    - ``build_dir`` is the directory where `hbmep` will store model artefacts, such as recruitment curve plots and estimates. If this directory does not already exists, `hbmep` will create it.\n",
    "- ``variables`` table contains the names of the columns in the dataset that are used by the model.\n",
    "    - ``intensity`` is the column name in the dataset that contains the TMS intensity.\n",
    "    - ``features`` is a list of columns that uniquely identify the set of data points for a single recruitment curve. In this case, the model will yield recruitment curve for each participant.\n",
    "    - ``response`` is a list of columns that contain the MEP size values. In this case, the model is run for two muscles, so the columns ``PKPK_ECR`` and ``PKPK_FCR`` are included in the list\n",
    "- ``[mcmc]`` table configures the No-U-Turn sampler (NUTS) sampler.\n",
    "    - ``num_chains`` is the number of chains to run in parallel.\n",
    "    - ``num_warmup`` is the number of warmup samples.\n",
    "    - ``num_samples`` is the number of samples to draw from the posterior.\n",
    "    - ``thinning`` is the thinning factor for the samples.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```{eval-rst}\n",
    ".. tip::\n",
    "\n",
    "    If you have trouble running the commands in this tutorial, please copy the command\n",
    "    and its output, then `open an issue <https://github.com/hbmep/hbmep/issues/new/choose>`_ on the `hbmep <https://github.com/hbmep/hbmep>`_ repository on\n",
    "    GitHub. We'll do our best to help you!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from hbmep.config import Config\n",
    "from hbmep.model.tms import RectifiedLogistic\n",
    "from hbmep.model.utils import Site as site\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by reading the `mock_data.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:\t(245, 4)\n",
      "Dataframe columns:\tTMSIntensity, participant, PKPK_ECR, PKPK_FCR\n",
      "All participants:\tP1, P2, P3\n",
      "\n",
      "Dataframe first 5 rows:\n",
      "\n",
      "   TMSIntensity participant  PKPK_ECR  PKPK_FCR\n",
      "0         43.79          P1     0.197     0.048\n",
      "1         55.00          P1     0.224     0.068\n",
      "2         41.00          P1     0.112     0.110\n",
      "3         43.00          P1     0.149     0.058\n",
      "4         14.00          P1     0.014     0.011\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./mock_data.csv\")\n",
    "\n",
    "print(f\"Dataframe shape:\\t{df.shape}\")\n",
    "print(f\"Dataframe columns:\\t{', '.join(df.columns.tolist())}\")\n",
    "print(f\"All participants:\\t{', '.join(df['participant'].unique().tolist())}\")\n",
    "\n",
    "n = 5\n",
    "print(f\"\\nDataframe first {n} rows:\\n\\n{df.head(n=n).to_string()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(build-the-model)=\n",
    "Now, we will build the model and process this dataset with it so that it's usable by _hbmep_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "config = Config(toml_path=\"./config.toml\")\n",
    "model = RectifiedLogistic(config=config)\n",
    "\n",
    "# Process data\n",
    "df, encoder_dict = model.load(df=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running inference, we can visualize the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "model.plot(df=df, encoder_dict=encoder_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "The above command created a pdf :file:`dataset.pdf` and saved it to the build directory. It can also be accessed here :download:`dataset.pdf <mock_data_output/dataset.pdf>`. As it can be seen, there are three participants with their MEPs recorded from two muscles.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run inference on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807fb773ba0149efaa37a2d8ee83f6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d934621d5934e608e2b367b827d55ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935bf67eb5d345ac9f4490183e7730ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55bc3b5798a4eef834eb7a2a4ec49a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run inference\n",
    "mcmc, posterior_samples = model.run_inference(df=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the convergence diagnostics with the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 mean       std    median      2.5%     97.5%     n_eff     r_hat\n",
      "     H[0,0]      0.24      0.01      0.24      0.21      0.27   2302.41      1.00\n",
      "     H[0,1]      0.16      0.01      0.16      0.14      0.18   3441.27      1.00\n",
      "     H[1,0]      0.19      0.04      0.18      0.14      0.26   1853.05      1.00\n",
      "     H[1,1]      0.16      0.07      0.14      0.08      0.29   1617.89      1.00\n",
      "     H[2,0]      0.23      0.05      0.22      0.15      0.34   3450.24      1.00\n",
      "     H[2,1]      0.26      0.13      0.22      0.10      0.52   2259.82      1.00\n",
      "    H_scale      0.28      0.13      0.25      0.12      0.54   1015.16      1.00\n",
      "     L[0,0]      0.01      0.00      0.01      0.01      0.02   3529.07      1.00\n",
      "     L[0,1]      0.01      0.00      0.01      0.01      0.01   3712.93      1.00\n",
      "     L[1,0]      0.01      0.00      0.01      0.01      0.01   4051.42      1.00\n",
      "     L[1,1]      0.01      0.00      0.01      0.01      0.01   3993.32      1.00\n",
      "     L[2,0]      0.01      0.00      0.01      0.00      0.01   4709.97      1.00\n",
      "     L[2,1]      0.01      0.00      0.01      0.01      0.01   4185.50      1.00\n",
      "    L_scale      0.01      0.01      0.01      0.01      0.02    690.70      1.00\n",
      "     a[0,0]     31.91      0.46     31.97     30.77     32.64    702.71      1.00\n",
      "     a[0,1]     31.59      0.53     31.67     30.60     32.59   2652.48      1.00\n",
      "     a[1,0]     45.59      0.81     45.79     44.22     46.47    340.41      1.00\n",
      "     a[1,1]     44.84      0.80     44.96     43.19     46.19   2482.68      1.00\n",
      "     a[2,0]     30.08      0.38     30.15     29.32     30.69   2455.03      1.00\n",
      "     a[2,1]     31.17      0.67     31.11     30.01     32.52   3348.39      1.00\n",
      "      a_loc     35.30      5.77     35.77     23.62     46.19   1216.14      1.00\n",
      "    a_scale     11.09      6.88      9.10      3.93     24.66   1071.15      1.00\n",
      "     b[0,0]      0.17      0.05      0.17      0.09      0.28   2313.03      1.00\n",
      "     b[0,1]      0.22      0.11      0.19      0.07      0.42   2782.91      1.00\n",
      "     b[1,0]      0.09      0.05      0.08      0.02      0.22    405.55      1.00\n",
      "     b[1,1]      0.07      0.04      0.07      0.01      0.15   2347.51      1.00\n",
      "     b[2,0]      0.09      0.04      0.08      0.03      0.15   2109.42      1.00\n",
      "     b[2,1]      0.05      0.03      0.04      0.01      0.10   2771.50      1.00\n",
      "    b_scale      0.18      0.09      0.15      0.05      0.34   1295.98      1.00\n",
      "  c_1_scale      3.16      2.79      2.35      0.08      8.92   2959.09      1.00\n",
      "  c_2_scale      0.29      0.14      0.26      0.12      0.53   1018.04      1.00\n",
      "    c₁[0,0]      2.66      3.40      1.47      0.05      9.25   3508.38      1.00\n",
      "    c₁[0,1]      2.70      3.38      1.48      0.04      9.41   3436.13      1.00\n",
      "    c₁[1,0]      2.37      3.37      1.14      0.01      9.00   3046.44      1.00\n",
      "    c₁[1,1]      2.50      3.36      1.27      0.03      9.13   3193.02      1.00\n",
      "    c₁[2,0]      1.36      2.82      0.10      0.01      7.32   1883.38      1.00\n",
      "    c₁[2,1]      2.38      3.15      1.17      0.03      8.78   3381.21      1.00\n",
      "    c₂[0,0]      0.07      0.01      0.07      0.05      0.10   4110.79      1.00\n",
      "    c₂[0,1]      0.13      0.02      0.12      0.09      0.17   4951.67      1.00\n",
      "    c₂[1,0]      0.08      0.02      0.08      0.06      0.12   3143.90      1.00\n",
      "    c₂[1,1]      0.23      0.04      0.22      0.16      0.30   4193.70      1.00\n",
      "    c₂[2,0]      0.32      0.12      0.29      0.16      0.57   1906.25      1.00\n",
      "    c₂[2,1]      0.34      0.06      0.33      0.23      0.45   3524.25      1.00\n",
      "  ell_scale      8.72      6.14      7.47      0.15     20.76   2873.60      1.00\n",
      "     ℓ[0,0]      7.74      8.14      4.85      0.08     24.39   3434.41      1.00\n",
      "     ℓ[0,1]      7.65      8.33      4.79      0.05     24.38   3215.37      1.00\n",
      "     ℓ[1,0]      6.41      8.07      3.49      0.00     23.11   2196.32      1.00\n",
      "     ℓ[1,1]      6.74      8.27      3.95      0.01     22.86   3548.49      1.00\n",
      "     ℓ[2,0]      6.80      8.06      4.02      0.01     23.35   3589.39      1.00\n",
      "     ℓ[2,1]      6.71      7.86      3.96      0.02     22.54   3877.42      1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.print_summary(samples=posterior_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "The different site names present in the first column of the above output correspond to model parameters (:cite:alps:`1987:nelson_`, see Methods 4.1.1, Supplementary Fig. ??)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the curves\n",
    "\n",
    "Now, we will plot the estimated recruitment curves. Before that, we need to use the estimated parameters and predict on a \"template\" prediction dataframe.\n",
    "\n",
    "This prediction dataframe is similar to the input dataframe on which inference was run, except it has missing MEP size response values. Additionally, it has many more intensity values (controlled by the argument `num_points=100`). These are the intensity values on which the model will predict the response values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction dataframe\n",
    "prediction_df = model.make_prediction_dataset(df=df, num_points=100)\n",
    "\n",
    "# Use the model to predict on the prediction dataframe\n",
    "posterior_predictive = model.predict(\n",
    "    df=prediction_df, posterior_samples=posterior_samples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the posterior predictive of the model. We can use this to plot the recruitment curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot recruitment curves\n",
    "model.render_recruitment_curves(\n",
    "    df=df,\n",
    "    encoder_dict=encoder_dict,\n",
    "    posterior_samples=posterior_samples,\n",
    "    prediction_df=prediction_df,\n",
    "    posterior_predictive=posterior_predictive\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "The above command created a pdf :file:`recruitment_curves.pdf` and saved it to the build directory. It can also be accessed here :download:`recruitment_curves.pdf <mock_data_output/recruitment_curves.pdf>`.\n",
    "\n",
    "As it can be seen in the pdf, the participants are aligned top to bottom, and the muscles are aligned left to right. The plots are colored by muscle name. For a given participant and muscle combination, the first plot shows the MEP size and stimulation intensity data, the second plot shows the estimated recruitment curve overlaid on top of the data, and the third plot shows posterior distribution of the threshold parameter.\n",
    "\n",
    "The estimated curves look good, except for participant P1 and muscle FCR, which is indexed by the tuple (0, 1). The curve seems to be biased towards a few outliers. Later on, we will see how to tackle this using a mixture model (:cite:alps:`1987:nelson_`, see Methods 4.1.1, Supplementary Fig. ??), but for now, we will go with it since the goal is to get comfortable with using `hbmep`.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can also plot the posterior predictive of the model using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check\n",
    "model.render_predictive_check(\n",
    "    df=df,\n",
    "    encoder_dict=encoder_dict,\n",
    "    prediction_df=prediction_df,\n",
    "    posterior_predictive=posterior_predictive\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "The above command created a pdf :file:`posterior_predictive_check.pdf` and saved it to the build directory. It can also be accessed here :download:`posterior_predictive_check.pdf <mock_data_output/posterior_predictive_check.pdf>`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters are stored in the `posterior_samples` dictionary. The keys are the parameter names and the values are the samples from the posterior distribution. The samples are stored as a NumPy array. The samples can be accessed as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a:\t(4000, 3, 2)\n",
      "\n",
      "First dimension corresponds to the number of samples:\t\t4000\n",
      "Second dimension corresponds to the number of participants:\t3\n",
      "Third dimension corresponds to the number of muscles:\t\t2\n"
     ]
    }
   ],
   "source": [
    "# Threshold parameter\n",
    "a = posterior_samples[site.a]\n",
    "\n",
    "print(f\"Shape of a:\\t{a.shape}\")\n",
    "print(f\"\\nFirst dimension corresponds to the number of samples:\\t\\t{a.shape[0]}\")\n",
    "print(f\"Second dimension corresponds to the number of participants:\\t{a.shape[1]}\")\n",
    "print(f\"Third dimension corresponds to the number of muscles:\\t\\t{a.shape[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 4000 samples because we ran 4 chains with 1000 samples each.\n",
    "\n",
    "Additionally, we can reshape the array so that the first dimension corresponds to the chains and the second dimension corresponds to the samples. Although this is not necessary, it can be useful for some analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a_grouped_by_chain:\t(4, 1000, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reshape samples to group them by chain\n",
    "a_grouped_by_chain = a.reshape(\n",
    "    model.mcmc_params[\"num_chains\"],\n",
    "    -1,\n",
    "    *a.shape[1:]\n",
    ")\n",
    "\n",
    "print(f\"Shape of a_grouped_by_chain:\\t{a_grouped_by_chain.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other curve parameters can be accessed in a similar way.\n",
    "\n",
    "<!-- However, there are two parameters which require a different kind of manipulation. Let's look at all the keys available and their shapes. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to:\t./mock_data_output/inference.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "destination_path = os.path.join(model.build_dir, \"inference.pkl\")\n",
    "\n",
    "with open(destination_path, \"wb\") as f:\n",
    "    pickle.dump((model, mcmc, posterior_samples,), f)\n",
    "\n",
    "print(f\"Model saved to:\\t{destination_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model can be loaded and used for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "source_path = destination_path\n",
    "\n",
    "with open(source_path, \"rb\") as f:\n",
    "    model, mcmc, posterior_samples = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using other functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "Alternatively, we can use other functions to estimate the recruitment curves. The available choices in `hbmep` (:cite:alps:`1987:nelson_`, see Results 2.3 and Fig. 3 for a comparison of their predictive performance to the rectified-logistic function, Methods 4.1, Eq. 4.1.1-4.1.3) are:\n",
    "```\n",
    "\n",
    "* Logistic-4, also known as the Boltzmann sigmoid.\n",
    "* Logistic-5, which is a more generalized version of Logistic4.\n",
    "* Rectified-linear.\n",
    "\n",
    "We recommend using Logistic-5 if estimating threshold is not the goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to use Logistic-5 function, we need to import it and modify the [code](build-the-model) appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic5 model\n",
    "from hbmep.model.tms import Logistic5\n",
    "\n",
    "\n",
    "# Build model\n",
    "config = Config(toml_path=\"./config.toml\")\n",
    "model = Logistic5(config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```{eval-rst}\n",
    ".. bibliography::\n",
    "    :cited:\n",
    "    :style: unsrt\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
